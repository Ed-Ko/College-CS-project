{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# ML HW4 Report\n",
    "\n",
    "## 1. data input\n",
    "\n",
    "Here I just use json library to read train.json and test.json in json format.\n",
    "\n",
    "And a one-hot encoding list is made for every ingredient and every cuisine(so one list for cuisine, one list for ingredient). The format of one-hot encoding list of cuisine is like below:\n",
    "\n",
    "italien : 0\n",
    "southern_us : 1\n",
    "thai : 2\n",
    "......\n",
    "......\n",
    "\n",
    "The mapped number of each cuisine or ingredient is the length of one-hot encoding list when the type is mapped."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. data preprocessing\n",
    "\n",
    "After turn train.json and test.json in json format, I change the format to nested list. Every ingredient is regarded as a feature and cuisines are regarded as targets in train.json. And for test.json, because there is no cuisine type as result, so the target(cuisine type) is replaced by the id of corresponding instance.\n",
    "\n",
    "eg for instance in train.json : ingredient, ingredient, ......... ingredient, cuisine\n",
    "\n",
    "eg for instance in test.json : ingredient, ingredient, ......... ingredient, id\n",
    "\n",
    "After that, I use one-hot encoding to map every type of cuisine and ingredient to the number(which is the current length of the one-hot encoding list then, so if the cuisine type \"italien\" is the first type to map to the one-hot encoding list for cuisine, then because the length of one-hot encoding list is 0, so \"italien\" is mapped to 0)\n",
    "\n",
    "Finally, I use train_test_split to split the data of train.json to trainning data and validation data with ratio 7:3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3. model construction & validation\n",
    "\n",
    "For SVM and ANN, I initially use the trainning data and validation data in data preprocessing to construct the model, but finally use the whole train.json to train the data, and predict two outcomes(one for validation data, one for test.json). \n",
    "\n",
    "And in this part, I also write the predicted outcome with higher accuracy to csv file for kaggle submission."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4. result\n",
    "\n",
    "The contents of result are represent in my code after running it.\n",
    "\n",
    "And because I finally use the whole train.json to train SVM and ANN, the accuracy is very high. But for the submission on Kaggle, the accuracy is about 74% to 76%."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5. comparison & conclusion\n",
    "For both SVM and ANN, because the dataset of trainning and testing are large enough to lower the efficiency of fitting, it takes a lot of time to test the effect of different value of various parameters to accuracy(SVM takes about 5 hours to test 4 kernel functions, while ANN takes much shorter).\n",
    "\n",
    "For SVM, I used 4 kernel(linear, rbf, poly and sigmoid) to test the accuracy. According to the result, polynomial and rbf are the two functions which usually have highest accuracy. BTW, because testing parameters takes too much time(it takes at least 1 day to fit the svm with 4 kernel functions), I just set regularization parameter(which is C in short) to 100 and not set kernel coefficient for rbf, poly and sigmoid.\n",
    "\n",
    "For ANN, I set three layers for the model, the input layer is a dense layer with input nueron number equals to the number of ingredient, and set output neuron number to half of the number of ingredient(That is, input neuron is about 6000 and output neuron is aobut 3000). The second layer is used to drop some neuron in order to prevent overfitting and will drop 20% of the neurons. The third layer(output layer) is the final calculation of probability of the current combination to its corresponding cuisine.\n",
    "\n",
    "To my surprise, ANN always has better accuracy and lower time cost than SVM, perhaps it's because I don't adjust the parameter of SVM to the best."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.  kaggle submission\n",
    "The picture is put in the compressed folder."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}